{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "\n",
    "Learning about how to fit generic mathematical functions in PyTorch, not just for neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to calibrate a new thermometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_celsius = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # celsius\n",
    "x = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # unknown shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_celsius = torch.tensor(truth_celsius)\n",
    "x = torch.tensor(x)\n",
    "\n",
    "type(truth_celsius), type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a linear model\n",
    "\n",
    "# celsius = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1265a3cc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x, truth_celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple model\n",
    "\n",
    "def model(data, w, b):\n",
    "    return data * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function - mean squared difference is used because it is better to have a few slightly\n",
    "# higher losses than a few really high ones, which mean squared error takes care of\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return torch.mean(torch.pow(y - y_pred, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "\n",
    "# we don't need to specify the shape of these params\n",
    "# because while using multiplication with pytorch tensors, they will be broadcasted\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "preds = model(x, w, b)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val = loss(preds, truth_celsius)\n",
    "loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis (forward pass) function is defined as:\n",
    "\n",
    "$$\n",
    "\\large h(x) = {w \\cdot x + b}\n",
    "$$\n",
    "\n",
    "To generalize the hypothesis to arbritrary parameters and inputs in a univariate linear regression, we derive:\n",
    "\n",
    "$$\n",
    "\\large h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "We will assume the following:\n",
    "\n",
    "$$\n",
    "\\large b = \\theta_0 \\\\\n",
    "\\large w = \\theta_1 \\\\\n",
    "$$\n",
    "\n",
    "Since $b$ or $\\theta_0$ is a constant/intercept, it will be multiplied with $1$.\n",
    "\n",
    "$$\n",
    "\\large x_0 = 1\n",
    "$$\n",
    "\n",
    "Hence the loss function will be:\n",
    "\n",
    "$$\n",
    "\\large \\hat{L}_{(y, \\hat{y})} = \\frac{1}{2m} \\sum_{i = 1}^{m}\\, (\\hat{y}^{(i)} - y^{(i)}) ^ 2 \\qquad ,y^{(i)} = h_{\\theta}(x^{(i)})\n",
    "$$\n",
    "\n",
    "Substituting the hypothesis function into the loss function:\n",
    "\n",
    "$$\n",
    "\\large \\hat{L}_{(\\theta_0, \\theta_1)} = \\frac{1}{2m} \\sum_{i = 1}^{m}\\, (h_{\\theta}(x^{(i)}) - y^{(i)}) ^ 2 \\qquad ,h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "__Note:__ The _2_ in the denominator of the loss function is placed there for convenience so that when the derivative of the loss function is taken, the _2's_ divide out and we are left with $\\large\\frac{1}{m}$\n",
    "however this is not implemented in the code because for optimization purposes it is unnecessary.\n",
    "\n",
    "\n",
    "The $\\large\\frac{1}{m}$ is in the equation so that the loss function does not depend on the number of examples in the dataset, and its values are comparable across any dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "The derivative of the loss function with respect to its parameters according to the Chain rule will be:\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial}{\\partial \\theta_{0, 1}}\\hat{L}(\\theta_0, \\theta_1) = \n",
    "\\frac{\\partial \\hat{L}(\\theta_0, \\theta_1)}{\\partial h_{\\theta}(x)} \\times\n",
    "\\frac{\\partial h_{\\theta}(x)}{\\partial \\theta_{0, 1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the loss function with respect to the hypothesis function is as follows:\n",
    "\n",
    "$$\n",
    "\\large\\frac{\\partial \\hat{L}(\\theta_0, \\theta_1)}{\\partial h_{\\theta}(x)} = \n",
    "\\frac{1}{m} \\sum_{i = 1}^{m}\\, (h_{\\theta}(x^{(i)}) - y^{(i)}) \\qquad , h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 \n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The derivative of the hypothesis function with respect to $\\theta_0$ will be:\n",
    "\n",
    "$$\n",
    "\\large\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_0} h_{\\theta}(x) = \\frac{\\partial}{\\partial \\theta_0} (\\theta_0 x_0 + \\theta_1 x_1) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_0} h_{\\theta}(x) = \\frac{\\partial}{\\partial \\theta_0} (\\theta_0 x_0) + \\frac{\\partial}{\\partial \\theta_0} (\\theta_1 x_1) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_0} h_{\\theta}(x) = \\frac{\\partial}{\\partial \\theta_0} (\\theta_0 x_0) + \\textit{0} \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_0} h_{\\theta}(x) = x_0 = 1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The value of $\\large\\frac{\\partial}{\\partial \\theta_0} h_{\\theta}(x)$ will be broadcasted to a vector so that it is compatible to be multiplied with $\\large\\frac{\\partial \\hat{L}(\\theta_0, \\theta_1)}{\\partial h_{\\theta}(x)}$.\n",
    "\n",
    "---\n",
    "\n",
    "The derivative of the hypothesis function with respect to $\\large\\theta_1$ will be:\n",
    "\n",
    "$$\n",
    "\\large\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_1} h_{\\theta}(x) = \\frac{\\partial}{\\partial \\theta_1} (\\theta_0 x_0 + \\theta_1 x_1) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} h_{\\theta}(x) = \\frac{\\partial}{\\partial \\theta_1} (\\theta_0 x_0) + \\frac{\\partial}{\\partial \\theta_1} (\\theta_1 x_1) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} h_{\\theta}(x) = \\textit{0} + \\frac{\\partial}{\\partial \\theta_1} (\\theta_1 x_1) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_1} h_{\\theta}(x) = x_1 \\\\\n",
    "x_1 = \\vec{x_1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\large\\vec{x_1}$ represents the input vector to the model (the list of temperature values in this case).\n",
    "\n",
    "---\n",
    "\n",
    "The gradient of the loss function will therefore be:\n",
    "\n",
    "$$\n",
    "\\Large\\nabla_{\\theta_0, \\theta_1} \\hat{L} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\hat{L}}{\\partial \\theta_0}, \\frac{\\partial \\hat{L}}{\\partial \\theta_1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\hat{L}}{\\partial h_{\\theta}(x)} \\frac{\\partial h_{\\theta}(x)}{\\partial \\theta_0},\n",
    "\\frac{\\partial \\hat{L}}{\\partial h_{\\theta}(x)} \\frac{\\partial h_{\\theta}(x)}{\\partial \\theta_1}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_h(preds, truth):\n",
    "    derivative = 2 * (preds - truth) / preds.size(0)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_w = lambda data, w, b: data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_b = lambda data, w, b: 1.0 # this will be broadcasted into a vector when multiplied with dl_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get all the gradients together\n",
    "\n",
    "def grad(data, preds, truth, w, b):\n",
    "    l = loss(preds, truth)\n",
    "    \n",
    "    dl_dh = dl_h(preds, truth) # this is an intermediate derivative. It's not of much use\n",
    "    \n",
    "    # these 2 statements result in vectors\n",
    "    dl_dw = dl_dh * dh_w(data, w, b)\n",
    "    dl_db = dl_dh * dh_b(data, w, b)\n",
    "    \n",
    "    return torch.stack([dl_dw.sum(), dl_db.sum()]) # sum is done to make the vectors into a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs, lr, params, data, truth):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        w, b = params\n",
    "        \n",
    "        curr_pred = model(data, w, b)\n",
    "        \n",
    "        curr_loss = loss(curr_pred, truth)\n",
    "        \n",
    "        gradient = grad(data, curr_pred, truth, w, b)\n",
    "        \n",
    "        params -= lr * gradient\n",
    "        \n",
    "        print(f\"Epoch {epoch}:\\tloss = {torch.round(curr_loss)}\\ngrad: dw: {torch.round(gradient[0])} db: {torch.round(gradient[1])}\\nparams: w: {params[0]} b: {params[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "          6.0000, 13.0000, 21.0000]),\n",
       " tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "         48.4000, 60.4000, 68.4000]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_celsius, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss = 1764.0\n",
      "grad: dw: 4517.0 db: 83.0\n",
      "params: w: -3.5172972679138184 b: -0.08260000497102737\n",
      "\n",
      "Epoch 2:\tloss = 41400.0\n",
      "grad: dw: -22060.0 db: -386.0\n",
      "params: w: 18.542882919311523 b: 0.3029572069644928\n",
      "\n",
      "Epoch 3:\tloss = 986625.0\n",
      "grad: dw: 107729.0 db: 1901.0\n",
      "params: w: -89.18596649169922 b: -1.597691535949707\n",
      "\n",
      "Epoch 4:\tloss = 23528174.0\n",
      "grad: dw: -526086.0 db: -9264.0\n",
      "params: w: 436.89971923828125 b: 7.666170120239258\n",
      "\n",
      "Epoch 5:\tloss = 561094592.0\n",
      "grad: dw: 2569098.0 db: 45257.0\n",
      "params: w: -2132.1982421875 b: -37.590972900390625\n",
      "\n",
      "Epoch 6:\tloss = 13380877312.0\n",
      "grad: dw: -12545989.0 db: -220992.0\n",
      "params: w: 10413.791015625 b: 183.40093994140625\n",
      "\n",
      "Epoch 7:\tloss = 319104516096.0\n",
      "grad: dw: 61267356.0 db: 1079214.0\n",
      "params: w: -50853.5703125 b: -895.8136596679688\n",
      "\n",
      "Epoch 8:\tloss = 7609945948160.0\n",
      "grad: dw: -299194336.0 db: -5270242.0\n",
      "params: w: 248340.78125 b: 4374.4287109375\n",
      "\n",
      "Epoch 9:\tloss = 181480487649280.0\n",
      "grad: dw: 1461091968.0 db: 25736834.0\n",
      "params: w: -1212751.25 b: -21362.40625\n",
      "\n",
      "Epoch 10:\tloss = 4327910768902144.0\n",
      "grad: dw: -7135127040.0 db: -125683768.0\n",
      "params: w: 5922376.0 b: 104321.3671875\n",
      "\n",
      "Epoch 11:\tloss = 1.0321114790939853e+17\n",
      "grad: dw: 34843832320.0 db: 613766784.0\n",
      "params: w: -28921456.0 b: -509445.4375\n",
      "\n",
      "Epoch 12:\tloss = 2.461359033357959e+18\n",
      "grad: dw: -170157113344.0 db: -2997281536.0\n",
      "params: w: 141235664.0 b: 2487836.25\n",
      "\n",
      "Epoch 13:\tloss = 5.869801758705425e+19\n",
      "grad: dw: 830948900864.0 db: 14636991488.0\n",
      "params: w: -689713280.0 b: -12149156.0\n",
      "\n",
      "Epoch 14:\tloss = 1.3998188285409708e+21\n",
      "grad: dw: -4057874300928.0 db: -71478591488.0\n",
      "params: w: 3368161280.0 b: 59329436.0\n",
      "\n",
      "Epoch 15:\tloss = 3.338262599310914e+22\n",
      "grad: dw: 19816312209408.0 db: 349060202496.0\n",
      "params: w: -16448151552.0 b: -289730784.0\n",
      "\n",
      "Epoch 16:\tloss = 7.961022428790051e+23\n",
      "grad: dw: -96771401318400.0 db: -1704607875072.0\n",
      "params: w: 80323256320.0 b: 1414877184.0\n",
      "\n",
      "Epoch 17:\tloss = 1.8985295614019997e+25\n",
      "grad: dw: 472575587123200.0 db: 8324319281152.0\n",
      "params: w: -392252358656.0 b: -6909442560.0\n",
      "\n",
      "Epoch 18:\tloss = 4.527576982018665e+26\n",
      "grad: dw: -2307786054565888.0 db: -40651165007872.0\n",
      "params: w: 1915533918208.0 b: 33741723648.0\n",
      "\n",
      "Epoch 19:\tloss = 1.0797280117197434e+28\n",
      "grad: dw: 1.1269893252972544e+16 db: 198516810973184.0\n",
      "params: w: -9354359078912.0 b: -164775100416.0\n",
      "\n",
      "Epoch 20:\tloss = 2.574914148345635e+29\n",
      "grad: dw: -5.503563362153267e+16 db: -969441227571200.0\n",
      "params: w: 45681276354560.0 b: 804666212352.0\n",
      "\n",
      "Epoch 21:\tloss = 6.140605718892351e+30\n",
      "grad: dw: 2.6876223429174886e+17 db: 4734189979041792.0\n",
      "params: w: -223080970452992.0 b: -3929523879936.0\n",
      "\n",
      "Epoch 22:\tloss = 1.464400182520412e+32\n",
      "grad: dw: -1.3124793334948168e+18 db: -2.311904874515661e+16\n",
      "params: w: 1089398456188928.0 b: 19189525905408.0\n",
      "\n",
      "Epoch 23:\tloss = 3.492273107180462e+33\n",
      "grad: dw: 6.409390828376031e+18 db: 1.1290006155322982e+17\n",
      "params: w: -5319992379703296.0 b: -93710540865536.0\n",
      "\n",
      "Epoch 24:\tloss = 8.32830697752647e+34\n",
      "grad: dw: -3.1299757925480792e+19 db: -5.513385952359219e+17\n",
      "params: w: 2.5979767547232256e+16 b: 457628060745728.0\n",
      "\n",
      "Epoch 25:\tloss = 1.9861188323954833e+36\n",
      "grad: dw: 1.52849937145327e+20 db: 2.69241920291183e+18\n",
      "params: w: -1.2687018141784474e+17 b: -2234791340539904.0\n",
      "\n",
      "Epoch 26:\tloss = inf\n",
      "grad: dw: -7.464308243650367e+20 db: -1.314822062920119e+19\n",
      "params: w: 6.195607147590451e+17 b: 1.0913429221015552e+16\n",
      "\n",
      "Epoch 27:\tloss = inf\n",
      "grad: dw: 3.645136977200014e+21 db: 6.420831686667692e+19\n",
      "params: w: -3.02557644880819e+18 b: -5.329488767143117e+16\n",
      "\n",
      "Epoch 28:\tloss = inf\n",
      "grad: dw: -1.7800742113659993e+22 db: -3.1355629828748555e+20\n",
      "params: w: 1.4775165785798083e+19 b: 2.6026142886107546e+17\n",
      "\n",
      "Epoch 29:\tloss = inf\n",
      "grad: dw: 8.692852504350159e+22 db: 1.5312276732181542e+21\n",
      "params: w: -7.215336589875374e+19 b: -1.2709663099154596e+18\n",
      "\n",
      "Epoch 30:\tloss = inf\n",
      "grad: dw: -4.245086883863936e+23 db: -7.477631017248035e+21\n",
      "params: w: 3.523553368428637e+20 b: 6.206665073469948e+18\n",
      "\n",
      "Epoch 31:\tloss = inf\n",
      "grad: dw: 2.0730550429514566e+24 db: 3.6516428772217136e+22\n",
      "params: w: -1.7206998797786536e+21 b: -3.0309766451924304e+19\n",
      "\n",
      "Epoch 32:\tloss = inf\n",
      "grad: dw: -1.0123603123610922e+25 db: -1.7832512532051297e+23\n",
      "params: w: 8.402903441990652e+21 b: 1.4801535812971685e+20\n",
      "\n",
      "Epoch 33:\tloss = inf\n",
      "grad: dw: 4.943782290817779e+25 db: 8.708368485930361e+23\n",
      "params: w: -4.103492180355535e+22 b: -7.22821547743477e+20\n",
      "\n",
      "Epoch 34:\tloss = inf\n",
      "grad: dw: -2.4142574756139368e+26 db: -4.2526639560498065e+24\n",
      "params: w: 2.003908359539879e+23 b: 3.529842574939516e+21\n",
      "\n",
      "Epoch 35:\tloss = inf\n",
      "grad: dw: 1.1789837287169705e+27 db: 2.0767552004053042e+25\n",
      "params: w: -9.785929996964521e+23 b: -1.7237709969545482e+22\n",
      "\n",
      "Epoch 36:\tloss = inf\n",
      "grad: dw: -5.757474978757671e+27 db: -1.0141671248262518e+26\n",
      "params: w: 4.7788821185647205e+24 b: 8.417900541339786e+22\n",
      "\n",
      "Epoch 37:\tloss = inf\n",
      "grad: dw: 2.8116176681436746e+28 db: 4.952605460741955e+26\n",
      "params: w: -2.3337296439828234e+25 b: -4.110815483709602e+23\n",
      "\n",
      "Epoch 38:\tloss = inf\n",
      "grad: dw: -1.3730316438928763e+29 db: -2.4185663398816502e+27\n",
      "params: w: 1.1396587567864517e+26 b: 2.0074849382199514e+24\n",
      "\n",
      "Epoch 39:\tloss = inf\n",
      "grad: dw: 6.705092474159559e+29 db: 1.181088059493923e+28\n",
      "params: w: -5.5654339689866966e+26 b: -9.803395316607434e+24\n",
      "\n",
      "Epoch 40:\tloss = inf\n",
      "grad: dw: -3.274379209203682e+30 db: -5.767750612106071e+28\n",
      "params: w: 2.717836133130785e+27 b: 4.7874115471479525e+25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING: BRACE FOR FAILURE\n",
    "train_loop(40, 0.001, torch.tensor([1.0, 0.0]), x, truth_celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See kids, this is why you don't set the learning rate to be too high. The training blows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss = 1764.0\n",
      "grad: dw: 4517.0 db: 83.0\n",
      "params: w: 0.5482703447341919 b: -0.008259999565780163\n",
      "\n",
      "Epoch 2:\tloss = 323.0\n",
      "grad: dw: 1860.0 db: 36.0\n",
      "params: w: 0.36231541633605957 b: -0.011838428676128387\n",
      "\n",
      "Epoch 3:\tloss = 79.0\n",
      "grad: dw: 765.0 db: 17.0\n",
      "params: w: 0.2857687473297119 b: -0.013489648699760437\n",
      "\n",
      "Epoch 4:\tloss = 38.0\n",
      "grad: dw: 315.0 db: 9.0\n",
      "params: w: 0.2542608380317688 b: -0.014347515068948269\n",
      "\n",
      "Epoch 5:\tloss = 31.0\n",
      "grad: dw: 130.0 db: 5.0\n",
      "params: w: 0.2412935048341751 b: -0.014878788031637669\n",
      "\n",
      "Epoch 6:\tloss = 29.0\n",
      "grad: dw: 53.0 db: 4.0\n",
      "params: w: 0.2359585464000702 b: -0.015275613404810429\n",
      "\n",
      "Epoch 7:\tloss = 29.0\n",
      "grad: dw: 22.0 db: 3.0\n",
      "params: w: 0.23376551270484924 b: -0.015617088414728642\n",
      "\n",
      "Epoch 8:\tloss = 29.0\n",
      "grad: dw: 9.0 db: 3.0\n",
      "params: w: 0.23286586999893188 b: -0.015935774892568588\n",
      "\n",
      "Epoch 9:\tloss = 29.0\n",
      "grad: dw: 4.0 db: 3.0\n",
      "params: w: 0.2324986606836319 b: -0.016245078295469284\n",
      "\n",
      "Epoch 10:\tloss = 29.0\n",
      "grad: dw: 1.0 db: 3.0\n",
      "params: w: 0.23235063254833221 b: -0.016550514847040176\n",
      "\n",
      "Epoch 11:\tloss = 29.0\n",
      "grad: dw: 1.0 db: 3.0\n",
      "params: w: 0.23229283094406128 b: -0.016854356974363327\n",
      "\n",
      "Epoch 12:\tloss = 29.0\n",
      "grad: dw: 0.0 db: 3.0\n",
      "params: w: 0.2322721779346466 b: -0.01715753972530365\n",
      "\n",
      "Epoch 13:\tloss = 29.0\n",
      "grad: dw: 0.0 db: 3.0\n",
      "params: w: 0.23226679861545563 b: -0.017460448667407036\n",
      "\n",
      "Epoch 14:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.23226772248744965 b: -0.01776324026286602\n",
      "\n",
      "Epoch 15:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2322712540626526 b: -0.018065981566905975\n",
      "\n",
      "Epoch 16:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2322758436203003 b: -0.01836869865655899\n",
      "\n",
      "Epoch 17:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.23228086531162262 b: -0.01867140270769596\n",
      "\n",
      "Epoch 18:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.23228606581687927 b: -0.018974097445607185\n",
      "\n",
      "Epoch 19:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2322913408279419 b: -0.019276786595582962\n",
      "\n",
      "Epoch 20:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2322966605424881 b: -0.01957947015762329\n",
      "\n",
      "Epoch 21:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323019802570343 b: -0.019882148131728172\n",
      "\n",
      "Epoch 22:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323073148727417 b: -0.020184820517897606\n",
      "\n",
      "Epoch 23:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323126494884491 b: -0.020487487316131592\n",
      "\n",
      "Epoch 24:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323179692029953 b: -0.02079014852643013\n",
      "\n",
      "Epoch 25:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323233038187027 b: -0.02109280414879322\n",
      "\n",
      "Epoch 26:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323286384344101 b: -0.021395454183220863\n",
      "\n",
      "Epoch 27:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323339730501175 b: -0.021698100492358208\n",
      "\n",
      "Epoch 28:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323392927646637 b: -0.022000741213560104\n",
      "\n",
      "Epoch 29:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323446273803711 b: -0.022303376346826553\n",
      "\n",
      "Epoch 30:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323499619960785 b: -0.022606005892157555\n",
      "\n",
      "Epoch 31:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323552817106247 b: -0.022908629849553108\n",
      "\n",
      "Epoch 32:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323606163263321 b: -0.023211248219013214\n",
      "\n",
      "Epoch 33:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323659509420395 b: -0.023513861000537872\n",
      "\n",
      "Epoch 34:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323712855577469 b: -0.023816470056772232\n",
      "\n",
      "Epoch 35:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323766052722931 b: -0.024119073525071144\n",
      "\n",
      "Epoch 36:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323819398880005 b: -0.02442167140543461\n",
      "\n",
      "Epoch 37:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.23238727450370789 b: -0.024724263697862625\n",
      "\n",
      "Epoch 38:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323925942182541 b: -0.025026850402355194\n",
      "\n",
      "Epoch 39:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.2323979288339615 b: -0.025329431518912315\n",
      "\n",
      "Epoch 40:\tloss = 29.0\n",
      "grad: dw: -0.0 db: 3.0\n",
      "params: w: 0.23240326344966888 b: -0.025632008910179138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(40, 1e-4, torch.tensor([1.0, 0.0]), x, truth_celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now kids, this is what we call converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try input normalization for the input vector so that the gradients (w and b) are not different from each other. This will allow the learning rate to meaningfully update both parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5700, 5.5900, 5.8200, 8.1900, 5.6300, 4.8900, 3.3900, 2.1800, 4.8400,\n",
       "         6.0400, 6.8400]),\n",
       " tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "          6.0000, 13.0000, 21.0000]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_n = 0.1 * x\n",
    "x_n, truth_celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss = 80.0\n",
      "grad: dw: -78.0 db: -11.0\n",
      "params: w: 1.0077613592147827 b: 0.0010639999527484179\n",
      "\n",
      "Epoch 2:\tloss = 80.0\n",
      "grad: dw: -77.0 db: -11.0\n",
      "params: w: 1.0154759883880615 b: 0.0021197462920099497\n",
      "\n",
      "Epoch 3:\tloss = 79.0\n",
      "grad: dw: -77.0 db: -10.0\n",
      "params: w: 1.0231441259384155 b: 0.003167289076372981\n",
      "\n",
      "Epoch 4:\tloss = 79.0\n",
      "grad: dw: -76.0 db: -10.0\n",
      "params: w: 1.0307661294937134 b: 0.004206678364425898\n",
      "\n",
      "Epoch 5:\tloss = 78.0\n",
      "grad: dw: -76.0 db: -10.0\n",
      "params: w: 1.0383422374725342 b: 0.005237963050603867\n",
      "\n",
      "Epoch 6:\tloss = 77.0\n",
      "grad: dw: -75.0 db: -10.0\n",
      "params: w: 1.045872688293457 b: 0.006261192727833986\n",
      "\n",
      "Epoch 7:\tloss = 77.0\n",
      "grad: dw: -75.0 db: -10.0\n",
      "params: w: 1.0533578395843506 b: 0.007276416290551424\n",
      "\n",
      "Epoch 8:\tloss = 76.0\n",
      "grad: dw: -74.0 db: -10.0\n",
      "params: w: 1.060797929763794 b: 0.00828368216753006\n",
      "\n",
      "Epoch 9:\tloss = 76.0\n",
      "grad: dw: -74.0 db: -10.0\n",
      "params: w: 1.0681931972503662 b: 0.009283038787543774\n",
      "\n",
      "Epoch 10:\tloss = 75.0\n",
      "grad: dw: -74.0 db: -10.0\n",
      "params: w: 1.0755438804626465 b: 0.010274534113705158\n",
      "\n",
      "Epoch 11:\tloss = 75.0\n",
      "grad: dw: -73.0 db: -10.0\n",
      "params: w: 1.0828503370285034 b: 0.011258215643465519\n",
      "\n",
      "Epoch 12:\tloss = 74.0\n",
      "grad: dw: -73.0 db: -10.0\n",
      "params: w: 1.0901128053665161 b: 0.012234130874276161\n",
      "\n",
      "Epoch 13:\tloss = 73.0\n",
      "grad: dw: -72.0 db: -10.0\n",
      "params: w: 1.0973315238952637 b: 0.01320232730358839\n",
      "\n",
      "Epoch 14:\tloss = 73.0\n",
      "grad: dw: -72.0 db: -10.0\n",
      "params: w: 1.1045067310333252 b: 0.014162851497530937\n",
      "\n",
      "Epoch 15:\tloss = 72.0\n",
      "grad: dw: -71.0 db: -10.0\n",
      "params: w: 1.1116387844085693 b: 0.015115750022232533\n",
      "\n",
      "Epoch 16:\tloss = 72.0\n",
      "grad: dw: -71.0 db: -9.0\n",
      "params: w: 1.1187279224395752 b: 0.016061069443821907\n",
      "\n",
      "Epoch 17:\tloss = 71.0\n",
      "grad: dw: -70.0 db: -9.0\n",
      "params: w: 1.1257743835449219 b: 0.016998855397105217\n",
      "\n",
      "Epoch 18:\tloss = 71.0\n",
      "grad: dw: -70.0 db: -9.0\n",
      "params: w: 1.1327784061431885 b: 0.01792915351688862\n",
      "\n",
      "Epoch 19:\tloss = 70.0\n",
      "grad: dw: -70.0 db: -9.0\n",
      "params: w: 1.139740228652954 b: 0.018852008506655693\n",
      "\n",
      "Epoch 20:\tloss = 70.0\n",
      "grad: dw: -69.0 db: -9.0\n",
      "params: w: 1.1466602087020874 b: 0.01976746693253517\n",
      "\n",
      "Epoch 21:\tloss = 69.0\n",
      "grad: dw: -69.0 db: -9.0\n",
      "params: w: 1.1535385847091675 b: 0.020675573498010635\n",
      "\n",
      "Epoch 22:\tloss = 69.0\n",
      "grad: dw: -68.0 db: -9.0\n",
      "params: w: 1.1603754758834839 b: 0.021576372906565666\n",
      "\n",
      "Epoch 23:\tloss = 68.0\n",
      "grad: dw: -68.0 db: -9.0\n",
      "params: w: 1.1671712398529053 b: 0.022469907999038696\n",
      "\n",
      "Epoch 24:\tloss = 68.0\n",
      "grad: dw: -68.0 db: -9.0\n",
      "params: w: 1.1739261150360107 b: 0.023356225341558456\n",
      "\n",
      "Epoch 25:\tloss = 68.0\n",
      "grad: dw: -67.0 db: -9.0\n",
      "params: w: 1.1806403398513794 b: 0.02423536591231823\n",
      "\n",
      "Epoch 26:\tloss = 67.0\n",
      "grad: dw: -67.0 db: -9.0\n",
      "params: w: 1.1873141527175903 b: 0.025107376277446747\n",
      "\n",
      "Epoch 27:\tloss = 67.0\n",
      "grad: dw: -66.0 db: -9.0\n",
      "params: w: 1.1939477920532227 b: 0.02597229741513729\n",
      "\n",
      "Epoch 28:\tloss = 66.0\n",
      "grad: dw: -66.0 db: -9.0\n",
      "params: w: 1.200541615486145 b: 0.026830172166228294\n",
      "\n",
      "Epoch 29:\tloss = 66.0\n",
      "grad: dw: -66.0 db: -9.0\n",
      "params: w: 1.207095742225647 b: 0.02768104523420334\n",
      "\n",
      "Epoch 30:\tloss = 65.0\n",
      "grad: dw: -65.0 db: -8.0\n",
      "params: w: 1.2136104106903076 b: 0.028524957597255707\n",
      "\n",
      "Epoch 31:\tloss = 65.0\n",
      "grad: dw: -65.0 db: -8.0\n",
      "params: w: 1.220085859298706 b: 0.02936195209622383\n",
      "\n",
      "Epoch 32:\tloss = 64.0\n",
      "grad: dw: -64.0 db: -8.0\n",
      "params: w: 1.226522445678711 b: 0.030192071571946144\n",
      "\n",
      "Epoch 33:\tloss = 64.0\n",
      "grad: dw: -64.0 db: -8.0\n",
      "params: w: 1.2329202890396118 b: 0.03101535513997078\n",
      "\n",
      "Epoch 34:\tloss = 64.0\n",
      "grad: dw: -64.0 db: -8.0\n",
      "params: w: 1.2392796277999878 b: 0.03183184564113617\n",
      "\n",
      "Epoch 35:\tloss = 63.0\n",
      "grad: dw: -63.0 db: -8.0\n",
      "params: w: 1.245600700378418 b: 0.032641585916280746\n",
      "\n",
      "Epoch 36:\tloss = 63.0\n",
      "grad: dw: -63.0 db: -8.0\n",
      "params: w: 1.2518837451934814 b: 0.033444616943597794\n",
      "\n",
      "Epoch 37:\tloss = 62.0\n",
      "grad: dw: -62.0 db: -8.0\n",
      "params: w: 1.2581290006637573 b: 0.034240975975990295\n",
      "\n",
      "Epoch 38:\tloss = 62.0\n",
      "grad: dw: -62.0 db: -8.0\n",
      "params: w: 1.2643367052078247 b: 0.035030707716941833\n",
      "\n",
      "Epoch 39:\tloss = 62.0\n",
      "grad: dw: -62.0 db: -8.0\n",
      "params: w: 1.2705070972442627 b: 0.03581384941935539\n",
      "\n",
      "Epoch 40:\tloss = 61.0\n",
      "grad: dw: -61.0 db: -8.0\n",
      "params: w: 1.2766404151916504 b: 0.036590442061424255\n",
      "\n",
      "Epoch 41:\tloss = 61.0\n",
      "grad: dw: -61.0 db: -8.0\n",
      "params: w: 1.2827367782592773 b: 0.03736052289605141\n",
      "\n",
      "Epoch 42:\tloss = 60.0\n",
      "grad: dw: -61.0 db: -8.0\n",
      "params: w: 1.2887965440750122 b: 0.03812413662672043\n",
      "\n",
      "Epoch 43:\tloss = 60.0\n",
      "grad: dw: -60.0 db: -8.0\n",
      "params: w: 1.2948198318481445 b: 0.038881316781044006\n",
      "\n",
      "Epoch 44:\tloss = 60.0\n",
      "grad: dw: -60.0 db: -8.0\n",
      "params: w: 1.300806999206543 b: 0.03963210806250572\n",
      "\n",
      "Epoch 45:\tloss = 59.0\n",
      "grad: dw: -60.0 db: -7.0\n",
      "params: w: 1.306758165359497 b: 0.04037654399871826\n",
      "\n",
      "Epoch 46:\tloss = 59.0\n",
      "grad: dw: -59.0 db: -7.0\n",
      "params: w: 1.312673568725586 b: 0.04111466556787491\n",
      "\n",
      "Epoch 47:\tloss = 59.0\n",
      "grad: dw: -59.0 db: -7.0\n",
      "params: w: 1.3185533285140991 b: 0.041846513748168945\n",
      "\n",
      "Epoch 48:\tloss = 58.0\n",
      "grad: dw: -58.0 db: -7.0\n",
      "params: w: 1.3243978023529053 b: 0.04257212206721306\n",
      "\n",
      "Epoch 49:\tloss = 58.0\n",
      "grad: dw: -58.0 db: -7.0\n",
      "params: w: 1.330207109451294 b: 0.04329153150320053\n",
      "\n",
      "Epoch 50:\tloss = 58.0\n",
      "grad: dw: -58.0 db: -7.0\n",
      "params: w: 1.3359814882278442 b: 0.04400477930903435\n",
      "\n",
      "Epoch 51:\tloss = 57.0\n",
      "grad: dw: -57.0 db: -7.0\n",
      "params: w: 1.3417211771011353 b: 0.04471190273761749\n",
      "\n",
      "Epoch 52:\tloss = 57.0\n",
      "grad: dw: -57.0 db: -7.0\n",
      "params: w: 1.347426414489746 b: 0.04541293531656265\n",
      "\n",
      "Epoch 53:\tloss = 57.0\n",
      "grad: dw: -57.0 db: -7.0\n",
      "params: w: 1.3530973196029663 b: 0.04610791802406311\n",
      "\n",
      "Epoch 54:\tloss = 56.0\n",
      "grad: dw: -56.0 db: -7.0\n",
      "params: w: 1.358734130859375 b: 0.04679688811302185\n",
      "\n",
      "Epoch 55:\tloss = 56.0\n",
      "grad: dw: -56.0 db: -7.0\n",
      "params: w: 1.3643370866775513 b: 0.04747987911105156\n",
      "\n",
      "Epoch 56:\tloss = 56.0\n",
      "grad: dw: -56.0 db: -7.0\n",
      "params: w: 1.3699064254760742 b: 0.04815692827105522\n",
      "\n",
      "Epoch 57:\tloss = 55.0\n",
      "grad: dw: -55.0 db: -7.0\n",
      "params: w: 1.3754422664642334 b: 0.04882807284593582\n",
      "\n",
      "Epoch 58:\tloss = 55.0\n",
      "grad: dw: -55.0 db: -7.0\n",
      "params: w: 1.380944848060608 b: 0.049493350088596344\n",
      "\n",
      "Epoch 59:\tloss = 55.0\n",
      "grad: dw: -55.0 db: -7.0\n",
      "params: w: 1.3864144086837769 b: 0.050152793526649475\n",
      "\n",
      "Epoch 60:\tloss = 54.0\n",
      "grad: dw: -54.0 db: -7.0\n",
      "params: w: 1.3918510675430298 b: 0.0508064366877079\n",
      "\n",
      "Epoch 61:\tloss = 54.0\n",
      "grad: dw: -54.0 db: -6.0\n",
      "params: w: 1.3972551822662354 b: 0.051454316824674606\n",
      "\n",
      "Epoch 62:\tloss = 54.0\n",
      "grad: dw: -54.0 db: -6.0\n",
      "params: w: 1.4026267528533936 b: 0.052096471190452576\n",
      "\n",
      "Epoch 63:\tloss = 54.0\n",
      "grad: dw: -53.0 db: -6.0\n",
      "params: w: 1.407966136932373 b: 0.0527329295873642\n",
      "\n",
      "Epoch 64:\tloss = 53.0\n",
      "grad: dw: -53.0 db: -6.0\n",
      "params: w: 1.4132734537124634 b: 0.053363729268312454\n",
      "\n",
      "Epoch 65:\tloss = 53.0\n",
      "grad: dw: -53.0 db: -6.0\n",
      "params: w: 1.418548822402954 b: 0.053988903760910034\n",
      "\n",
      "Epoch 66:\tloss = 53.0\n",
      "grad: dw: -52.0 db: -6.0\n",
      "params: w: 1.4237926006317139 b: 0.05460849031805992\n",
      "\n",
      "Epoch 67:\tloss = 52.0\n",
      "grad: dw: -52.0 db: -6.0\n",
      "params: w: 1.4290049076080322 b: 0.0552225187420845\n",
      "\n",
      "Epoch 68:\tloss = 52.0\n",
      "grad: dw: -52.0 db: -6.0\n",
      "params: w: 1.4341858625411987 b: 0.055831026285886765\n",
      "\n",
      "Epoch 69:\tloss = 52.0\n",
      "grad: dw: -51.0 db: -6.0\n",
      "params: w: 1.4393357038497925 b: 0.05643404275178909\n",
      "\n",
      "Epoch 70:\tloss = 52.0\n",
      "grad: dw: -51.0 db: -6.0\n",
      "params: w: 1.4444546699523926 b: 0.05703160539269447\n",
      "\n",
      "Epoch 71:\tloss = 51.0\n",
      "grad: dw: -51.0 db: -6.0\n",
      "params: w: 1.4495428800582886 b: 0.05762374401092529\n",
      "\n",
      "Epoch 72:\tloss = 51.0\n",
      "grad: dw: -51.0 db: -6.0\n",
      "params: w: 1.4546005725860596 b: 0.05821049213409424\n",
      "\n",
      "Epoch 73:\tloss = 51.0\n",
      "grad: dw: -50.0 db: -6.0\n",
      "params: w: 1.4596278667449951 b: 0.058791883289813995\n",
      "\n",
      "Epoch 74:\tloss = 51.0\n",
      "grad: dw: -50.0 db: -6.0\n",
      "params: w: 1.4646250009536743 b: 0.05936795100569725\n",
      "\n",
      "Epoch 75:\tloss = 50.0\n",
      "grad: dw: -50.0 db: -6.0\n",
      "params: w: 1.4695922136306763 b: 0.05993872508406639\n",
      "\n",
      "Epoch 76:\tloss = 50.0\n",
      "grad: dw: -49.0 db: -6.0\n",
      "params: w: 1.4745296239852905 b: 0.0605042390525341\n",
      "\n",
      "Epoch 77:\tloss = 50.0\n",
      "grad: dw: -49.0 db: -6.0\n",
      "params: w: 1.4794373512268066 b: 0.061064526438713074\n",
      "\n",
      "Epoch 78:\tloss = 50.0\n",
      "grad: dw: -49.0 db: -6.0\n",
      "params: w: 1.4843156337738037 b: 0.06161961704492569\n",
      "\n",
      "Epoch 79:\tloss = 49.0\n",
      "grad: dw: -48.0 db: -5.0\n",
      "params: w: 1.4891645908355713 b: 0.06216954067349434\n",
      "\n",
      "Epoch 80:\tloss = 49.0\n",
      "grad: dw: -48.0 db: -5.0\n",
      "params: w: 1.4939844608306885 b: 0.06271433085203171\n",
      "\n",
      "Epoch 81:\tloss = 49.0\n",
      "grad: dw: -48.0 db: -5.0\n",
      "params: w: 1.4987754821777344 b: 0.06325402110815048\n",
      "\n",
      "Epoch 82:\tloss = 49.0\n",
      "grad: dw: -48.0 db: -5.0\n",
      "params: w: 1.5035377740859985 b: 0.06378863751888275\n",
      "\n",
      "Epoch 83:\tloss = 48.0\n",
      "grad: dw: -47.0 db: -5.0\n",
      "params: w: 1.5082714557647705 b: 0.0643182173371315\n",
      "\n",
      "Epoch 84:\tloss = 48.0\n",
      "grad: dw: -47.0 db: -5.0\n",
      "params: w: 1.5129767656326294 b: 0.06484278291463852\n",
      "\n",
      "Epoch 85:\tloss = 48.0\n",
      "grad: dw: -47.0 db: -5.0\n",
      "params: w: 1.5176538228988647 b: 0.0653623715043068\n",
      "\n",
      "Epoch 86:\tloss = 48.0\n",
      "grad: dw: -46.0 db: -5.0\n",
      "params: w: 1.5223028659820557 b: 0.06587701290845871\n",
      "\n",
      "Epoch 87:\tloss = 48.0\n",
      "grad: dw: -46.0 db: -5.0\n",
      "params: w: 1.5269240140914917 b: 0.06638672947883606\n",
      "\n",
      "Epoch 88:\tloss = 47.0\n",
      "grad: dw: -46.0 db: -5.0\n",
      "params: w: 1.531517505645752 b: 0.06689155846834183\n",
      "\n",
      "Epoch 89:\tloss = 47.0\n",
      "grad: dw: -46.0 db: -5.0\n",
      "params: w: 1.536083459854126 b: 0.0673915296792984\n",
      "\n",
      "Epoch 90:\tloss = 47.0\n",
      "grad: dw: -45.0 db: -5.0\n",
      "params: w: 1.5406219959259033 b: 0.06788666546344757\n",
      "\n",
      "Epoch 91:\tloss = 47.0\n",
      "grad: dw: -45.0 db: -5.0\n",
      "params: w: 1.545133352279663 b: 0.06837700307369232\n",
      "\n",
      "Epoch 92:\tloss = 46.0\n",
      "grad: dw: -45.0 db: -5.0\n",
      "params: w: 1.5496176481246948 b: 0.06886257231235504\n",
      "\n",
      "Epoch 93:\tloss = 46.0\n",
      "grad: dw: -45.0 db: -5.0\n",
      "params: w: 1.5540751218795776 b: 0.06934339553117752\n",
      "\n",
      "Epoch 94:\tloss = 46.0\n",
      "grad: dw: -44.0 db: -5.0\n",
      "params: w: 1.558505892753601 b: 0.06981950253248215\n",
      "\n",
      "Epoch 95:\tloss = 46.0\n",
      "grad: dw: -44.0 db: -5.0\n",
      "params: w: 1.5629100799560547 b: 0.07029092311859131\n",
      "\n",
      "Epoch 96:\tloss = 46.0\n",
      "grad: dw: -44.0 db: -5.0\n",
      "params: w: 1.567287802696228 b: 0.07075768709182739\n",
      "\n",
      "Epoch 97:\tloss = 45.0\n",
      "grad: dw: -44.0 db: -5.0\n",
      "params: w: 1.5716392993927002 b: 0.07121982425451279\n",
      "\n",
      "Epoch 98:\tloss = 45.0\n",
      "grad: dw: -43.0 db: -5.0\n",
      "params: w: 1.5759648084640503 b: 0.07167736440896988\n",
      "\n",
      "Epoch 99:\tloss = 45.0\n",
      "grad: dw: -43.0 db: -5.0\n",
      "params: w: 1.5802643299102783 b: 0.07213032990694046\n",
      "\n",
      "Epoch 100:\tloss = 45.0\n",
      "grad: dw: -43.0 db: -4.0\n",
      "params: w: 1.5845381021499634 b: 0.07257875055074692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(100, 1e-4, torch.tensor([1.0, 0.0]), x_n, truth_celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
